# Model

# 22 Introduction

# Now that you are equipped with powerful programming tools we can finally return to modeling.
# You'll use your new tools of data wrangling and programming, to fit many nmodels and understand how they work.
# The focus of this book is on ecploration, not confirmation or formal inference.
# But you'll learn a few basic tools that help you understand the variation within your models.

#  /-----------------------------------------------------------------\
#  |                                                                 |
#  |                   /---------------------------\                 |
#  |                   |         --> Visalise -->  |                 |
#  |                   |        |               |  |                 |
#  | Import --> Tidy --|--> Transform           |  |--> Communicate  |
#  |                   |        |               |  |                 |
#  |                   |        <---  Model <---   |                 |
#  |                   \---------------------------/                 |
#  |                   Understand                                    |
#  \-----------------------------------------------------------------/
#  Program

# The goal of a model is to provide a simple low-dimensional summary of a dataset.
# Ideally, the model will capture true "signals" (i.e. patterns generated by the phenomenon of interest),
# and ignore "noise" (i.e. random variation that you're not interested in).
# Here we only cover "predictive" models, which, as the name suggests, generate predictions.
# There is another type of model that we're not going to discuss: "data discovery" models.
# These models don't make predictions, but instead help you discover interesting relationships within your data.
# (These two categories of models are sometimes calles supervised and unsupervised, but I don't think that terminology is particularly illuminating.)

# This book is not going to give you a deep understanding of the mathematical theory that underlies models.
# It will, however, build your intuition about how statistical models work, and give you a familyof useful tools 
# that allow you to use models to better understand your data:
# 1. In model basics, you'll how models work mechanistically, focussing on the important family of linear models.
#    You'll learn general tools for gaining insight into what a predictive model tells you about your data, 
#    focussing on simple sumulated datasets. (https://r4ds.had.co.nz/model-basics.html#model-basics)
# 2. In model building, you'll learn how to use models to pull out known patterns in real data.
#    Once you have recognised an important pattern it's useful to make it explicit in a model, 
#    because then you can more easily see the subtler signals that remain. (https://r4ds.had.co.nz/model-building.html#model-building)
# 3. In many models, you'll learn how to use many simple models to help understand complex datasets.
#    This is a powerful technique, but to access it you'll need to combine modelling and programming tools.
#    (https://r4ds.had.co.nz/many-models.html#many-models)

# These topics are notable because of what they don't include: any tools for quantitatively assessing models.
# That is deliberate: precisely quantifying a model requires a couple of big ideas that we just don't have the space to cover here.
# For now, you'll rely on qualitative assessment and your narural scepticism.
# In Learning more about models, we'll point you to other resources there you can learn more.
# (https://r4ds.had.co.nz/model-building.html#learning-more-about-models)

# 22.1 Hypothesis generation vs. hypothesis confirmation

# In this book, we are going to use moedls as a tool for exploration, completing the trifecta of the tools for EDA that were introduced in Part 1.
# This is not how models are usually taught, but as you will see, models are an important tool for exploration.
# Traditionally, the focus of modelling is on inference, or for confirming that an hypothesis is true.
# Doing this correctly is not complicated, but it is hard.
# There is a pair of ideas that you must understand in order to do inference correctly:
# 1. Each obsevation can either be used for exploration or confirmation, not both.
# 2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation.
#    AS soon as you use an observation twice, you've switched from confirmation to exploration.

# This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis.
# Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, 
# but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.

# If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces befor you begin the analysis:
# 1. 60% fo your data goes into a training (or exploration) set.
#    You're allowed to do anything you like with this data: visualise it and fit tons of models to it.
# 2. 20% goes into a query set. You can use this data to compare models or visualisations by hand, 
#    but you're not allowed to use it as part of an automated process.
# 3. 20% is held bach for a test set. You can oly use this data ONCE, to test your final model.

# This partitioning allows you to explore the training data, occasionally generating candidate hypotheses that you check with the query set.
# When you are confident you have the right model, you can check it once with the test data.

# (Note that even when doing confirmatory modelling, you will still need to do EDA.
# If you don't do any EDA you will remain blind to the quality problems with your data.)

# ----------------------------------------------------------------------------------------------------------------------------

# 23 Model basics

# 23.1 Introduction

# The goal of a model is to provide a simple low-dimensional summary of a dataset.
# In the context of this book we're going to use models to partition data into patterns and residuals.
# Strong patterns will hide subtler trends, so we'll use models to help peel back layers of structure as we explore a dataset.

# However, before we can start using models on interesting, real, datasets, you need to understand the basics of how models work.
# For that reason, this chapter of the book is unque because it uses only sumulated datasets.
# These datasets are very simple, and not at all interesting, but they will help you understand the essence of modeling 
# before you apply the same techniques to real data in the next chapter.

# There are two parts to a model:
# 1. First, you define a family of models that express a precise, but generic, pattern that you want to capture.
#    For example, the pattern might be a straight line, or a quadratic curve.
#    You will express the model family as an equation like y = a_1 * x + a_2 or y = a_1 * x ^ a_2.
#    Here, x and y are known variables from your data, and a_1 and a_2 are parameters that can vary to capture different patterns.
# 2. Next, you generate a fitted model by finding th model from the family that is the closest to your data.
#    This takes the generic model family and makes it specific, like y = 3 * x + 7 or y = 9 * x ^ 2.

# It's important to understand that a fitted model is just the closest model from a family of models.
# That implies that you have the "best" model(according to some criteria); 
# it doesn't imply that you have a good model and it certainly doesn't imply that the model is "true".
# George Box puts this well in his famous aphorism.

# '''
# All models are wrong, but some are useful. 
# '''

# It's worth reading the fuller context of the quote:

# ''' 
# Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model.
# However, cunningly chosen parsimonious models often do provide remarkably useful approximations.
# For example, the law PV = RT relating pressure P, volume V and temperature T of an "ideal" gas via a constant R is not exactly true for any real gas,
# but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

# For such a model there is no need to ask the question "Is the model true?". If "truth" is to be the "whole truth" the answer must be "No".
# The only question of interest is "Is the model illuminating and useful?". 
# '''

# The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.

# 23.1.1 Prerequisites 

# In this chapter we'll use the modelr package which wraps around base R's modelling functions to make them work naturally in a pipe.
library(tidyverse)

library(modelr)
options(na.action = na.warn)

# 23.2 A simple model

# Lets take a look at the simulated dataset sim1, included with the modelr package.
# It contains two continuous variables, x and y. Let's plot them to see how they're related:
ggplot(sim1, aes(x, y)) + 
  geom_point()

# You can see a strong pattern in the data. Let's use a model to capture that pattern and make it explicit.
# It's our job to supply the basic form of the model. In this case, the relationship looks linear, i.e. y = a_0 + a_1 * x.
# Let's start by getting a feel for what models from that family look like by randomly generating a few and overlaying them on the data.
# For this simple case, we can use geom_abline() which takes a slopt and intercept as parameters.
# Later on we'll learn more general techniques that work with any model.
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) + 
  geom_point()

# There are 250 models on this plot, but a lot are really bad! 
# We need to find the good models by making precise our intuition that a good model is "close" to the data.
# We need a way to quantify the distance between the data and a model.
# Then we can fit the model by finding the value of a_0 and a_1 that generate the model with the smallest distance from this data.

# One easy place to start is to find the vertical distance between each point and the model, as in the following diagram.
# (Note that I've shifted the x values slightly so you can see the individual distance.)

# This distance is just the difference between the y value given by the model (the prediction), 
# and the actual y value in the data (the response).

# To compute this distance, we first turn our model family into an R function.
# This takes the model parameters and the data as inputs, and givees values predicted by the model as output:
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)

# Next, we need some way to compute an overall distance between the predicted and actual values.
# In other words, the plot above shows 30 distances: how do we collapse that into a single number?

# One commmon way to do this in statistics to use the "root-mean-squared deviation".
# We compute the difference between actual and predicted, square them, average them, and the take the square root.
# This distance has lots of appealing mathematical properties, which we're not going to talk about here.
# You'll just have to take my word for it!
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)

















